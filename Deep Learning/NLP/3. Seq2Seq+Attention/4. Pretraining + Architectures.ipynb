{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41d025a2",
   "metadata": {},
   "source": [
    "### üìö Key Concepts in Pretraining\n",
    "\n",
    "- **Why Pretrain?**\n",
    "  - Pretraining allows models to learn from massive unlabeled data.\n",
    "  - It boosts performance across diverse NLP tasks with minimal task-specific supervision.\n",
    "  - The pretraining step boosts the model's intelligence across diverse tasks because of the magnitude of data that is available for pretraining (no labelling needed) and the variety within the data. While in finetuning, it is carefully curated, labelled data, the process is expensive and that results in way less data than what is available for pretraining.\n",
    "  - In terms of the mathematics of it, it is deemed to start the finetuning gradient descent with the weights learnt from pretraining as oppossed to starting from a random starting point. The starting point is extremely crucial when it comes to well known first order gradient descent algorithms.\n",
    "\n",
    "- **Subword Modeling**\n",
    "  - Fixed word vocabularies are limiting‚Äînovel words become `<UNK>`.\n",
    "  - Subword tokenization (e.g., Byte-Pair Encoding, WordPiece) helps handle rare and misspelled words.\n",
    "  - Words are split into meaningful subword units, improving generalization.\n",
    "\n",
    "- **Three Pretraining Architectures**\n",
    "  - **Encoder-only** (e.g., BERT): Good for understanding tasks like classification.\n",
    "  - **Decoder-only** (e.g., GPT): Suited for generation tasks like text completion.\n",
    "  - **Encoder-Decoder** (e.g., T5, BART): Ideal for translation and summarization.\n",
    "\n",
    "- **Distributional Semantics**\n",
    "  - ‚ÄúYou shall know a word by the company it keeps‚Äù ‚Äî motivates learning word meaning from context.\n",
    "  - This principle underlies models like word2vec and modern transformers.\n",
    "\n",
    "- **In-Context Learning**\n",
    "  - Large models can learn tasks from examples in the input without updating weights.\n",
    "  - This is a key capability of models like GPT-3 and beyond.\n",
    "\n",
    "- **Scaling Laws**\n",
    "  - Bigger models + more data + more compute = better performance.\n",
    "  - Pretraining benefits scale predictably with resources.\n",
    "\n",
    "---\n",
    "</br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd805e5",
   "metadata": {},
   "source": [
    "### The **pretraining task** is crucial because it shapes what each Transformer architecture learns to do best. Here are the key details:\n",
    "\n",
    "\n",
    "\n",
    "### üß† Transformer Pretraining Architectures & Their Tasks\n",
    "\n",
    "#### 1. **Encoder-only** (e.g., BERT)\n",
    "- **Architecture**: Only the encoder stack is used.\n",
    "- **Pretraining Task**:  \n",
    "  - **Masked Language Modeling (MLM)**: Since the architecture of encoders gets them bidirectional context, it doesn't make sense to do causal language modeling tasks with encoder only arch. The process of bidirectionality will allow the first step to be able to see the next word already. Therefore we go for Masked Language Modeling with this arch where random tokens are masked, and the model predicts them using the rest of the full context.\n",
    "- **Best For**:  \n",
    "  - Representational tasks, embedding generation and on top of that understanding tasks like classification, sentiment analysis, named entity recognition (NER), etc.\n",
    "- Tip: if you are going to use BERT, use RoBERTa due to similar arch but better training\n",
    "\n",
    "#### 2. **Decoder-only** (e.g., GPT)\n",
    "- **Architecture**: Only the decoder stack is used.\n",
    "- **Pretraining Task**:  \n",
    "  - **Causal Language Modeling (CLM)**: Predict the next token given previous ones (left-to-right).\n",
    "- **Best For**:  \n",
    "  - Generation tasks like story writing, code generation, dialogue, and text completion.\n",
    "\n",
    "#### 3. **Encoder-Decoder** (e.g., T5, BART)\n",
    "- **Architecture**: Full Transformer with both encoder and decoder.\n",
    "- **Pretraining Tasks**:\n",
    "  - **T5**: Text-to-text format using multiple tasks (e.g., translation, summarization, question answering).\n",
    "  - **BART**: Denoising autoencoder ‚Äî corrupt input and train to reconstruct it.\n",
    "- **Best For**:  \n",
    "  - Sequence-to-sequence tasks like translation, summarization, and question answering.\n",
    "\n",
    "---\n",
    "\n",
    "### üîë Takeaway\n",
    "\n",
    "- The **pretraining task** determines what the model learns:\n",
    "  - MLM ‚Üí deep understanding of context.\n",
    "  - CLM ‚Üí fluent generation.\n",
    "  - Denoising / text-to-text ‚Üí flexible input-output mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fdd744",
   "metadata": {},
   "source": [
    "Pretraining benefits:\n",
    "Tons of data as it is unlabelled and easy to get\n",
    "Variety of data\n",
    "makes the model more generalised\n",
    "A careful selection is need to make sure garbage doesn't go in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a6b7e2",
   "metadata": {},
   "source": [
    "### üß† FineTuning Insights\n",
    "\n",
    "When we want to take a pre-trained model and finetune it on our custom data, we don't want it to completely lose the knowledge it gained in the pretraining step. If we by default finetune all the parameters we end up losing a lot of valuable information learnt through expensive training step. Therefore, we would like to only tune a few parameters to make only enough change to the model to incorporate expertise on our data.\n",
    "This is where PEFT comes in.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
