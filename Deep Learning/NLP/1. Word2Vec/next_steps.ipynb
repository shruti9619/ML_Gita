{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9126279",
   "metadata": {},
   "source": [
    "# RNN, Attention, and Transformers Learning Path Revision Notebook\n",
    "\n",
    "A one-page flashcard-style revision tool explaining why understanding RNNs and attention is key to mastering Transformers after Word2Vec. Designed for quick, deep review without code.\n",
    "\n",
    "## 1. Why Learn RNNs and Attention Before Transformers?\n",
    "- **Progression**: Word2Vec (static embeddings) → Contextual models (RNNs, attention) → Transformers (state-of-the-art).\n",
    "- **Foundation**: RNNs introduce sequence processing, a precursor to Transformers' handling of text.\n",
    "- **Attention Bridge**: Attention builds on RNNs, enabling Transformers to focus on relevant words, critical for sentiment analysis.\n",
    "- **Complexity**: Transformers combine RNN ideas (sequence) and attention (context), so understanding both simplifies the leap.\n",
    "- **Practicality**: Sentiment tasks (e.g., sarcasm, negation) need sequential/contextual understanding, mastered via this path.\n",
    "\n",
    "## 2. What Are RNNs?\n",
    "- **Definition**: Neural networks that process sequences by maintaining a \"memory\" of previous inputs (e.g., words in a sentence).\n",
    "- **How They Work**:\n",
    "  - Updates hidden state $h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b)$ at each time step $t$.\n",
    "  - $x_t$: Current input (e.g., word vector), $h_{t-1}$: Previous state, $f$: Activation (e.g., tanh).\n",
    "- **Sentiment Use**: Captures order (e.g., \"not good\" as negative) via sequential memory.\n",
    "- **Limitations**: Vanishing/exploding gradients, struggles with long dependencies.\n",
    "\n",
    "## 3. What Are Advanced RNNs?\n",
    "- **LSTM (Long Short-Term Memory)**:\n",
    "  - Adds gates (forget, input, output) to control memory, solving long-term dependency issues.\n",
    "  - Sentiment Benefit: Remembers context over sentences (e.g., \"not\" affects \"good\").\n",
    "- **GRU (Gated Recurrent Unit)**:\n",
    "  - Simplified LSTM with update/reset gates, faster but less flexible.\n",
    "  - Sentiment Benefit: Efficient for shorter texts.\n",
    "- **Why Learn**: Base for bidirectional processing and attention integration.\n",
    "\n",
    "## 4. What is Attention?\n",
    "- **Definition**: Mechanism to focus on relevant parts of the input sequence, weighting importance of each word.\n",
    "- **How It Works**:\n",
    "  - Computes attention scores $a_{ij} = \\text{align}(h_i, h_j)$ between query $h_i$ (current state) and keys $h_j$ (all states).\n",
    "  - Normalizes to weights $w_{ij} = \\frac{\\exp(a_{ij})}{\\sum \\exp(a_{ik})}$, then weighted sum of values.\n",
    "- **Sentiment Use**: Highlights \"not\" in \"not good\" for accurate classification.\n",
    "- **Advantage**: Overcomes RNN’s long-dependency limit by direct word connections.\n",
    "\n",
    "## 5. How Do They Lead to Transformers?\n",
    "- **RNN Evolution**: Bidirectional RNNs + attention (e.g., Bahdanau 2014) improve translation, inspiring Transformers.\n",
    "- **Transformer Innovation**:\n",
    "  - Replaces RNNs with self-attention, processing all words simultaneously.\n",
    "  - Uses multi-head attention and feedforward layers, scaling to large data.\n",
    "- **Sentiment Impact**: Captures full context (e.g., sarcasm across sentences), outperforming RNNs.\n",
    "- **Why Learn Path**: RNNs teach sequence handling, attention adds focus, Transformers combine both efficiently.\n",
    "\n",
    "## 6. Learning Path & Tips\n",
    "- **Order**: Start with basic RNNs → LSTMs/GRUs → Attention → Transformers.\n",
    "- **Resources**: \n",
    "  - RNNs: \"Deep Learning\" by Goodfellow (Chapter 10).\n",
    "  - Attention: \"Attention is All You Need\" paper (Vaswani et al., 2017).\n",
    "  - Transformers: Hugging Face tutorials or \"The Illustrated Transformer\".\n",
    "- **Practice**: Apply to sentiment (e.g., classify \"I am not happy\" with RNN vs. Transformer).\n",
    "- **Goal**: Build intuition for BERT’s bidirectional attention, key for modern sentiment.\n",
    "\n",
    "## 7. Key Points (Milestones)\n",
    "- **W2V**: Static embeddings, ~2013.\n",
    "- **RNNs**: Sequence models, ~2015 (LSTMs).\n",
    "- **Attention**: Contextual focus, ~2017.\n",
    "- **Transformers**: Scalable context, 2017 onward (BERT, 2018).\n",
    "- **Sentiment Gain**: From ~80% (W2V) to >90% (Transformers) on benchmarks like SST-2."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
