{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5e2b0a3",
   "metadata": {},
   "source": [
    "# Skip-gram Word2Vec Revision Notebook\n",
    "\n",
    "A one-page flashcard-style revision tool for the Skip-gram model in Word2Vec. Covers architecture, why it works, dense embeddings vs. one-hot vectors, and negative sampling. Designed for quick, deep review without code.\n",
    "\n",
    "## 1. What is Skip-gram?\n",
    "- **Definition**: Predicts context words (e.g., \"quick\", \"brown\", \"jumps\") from a target word (e.g., \"fox\").\n",
    "- **Goal**: Learn dense word embeddings (vectors, e.g., 100-dim) that capture semantic/syntactic relationships (e.g., \"king\" ≈ \"queen\", \"king - man + woman ≈ queen\").\n",
    "- **Core Idea**: Distributional hypothesis—words in similar contexts have similar meanings.\n",
    "- **Training**: Unsupervised, uses large text corpora (e.g., Wikipedia), sliding window to generate (target, context) pairs.\n",
    "- **Example**: Sentence: \"The quick brown fox jumps.\" Target: \"fox\". Context (window size 2): \"quick\", \"brown\", \"jumps\", \"over\".\n",
    "\n",
    "## 2. Skip-gram Architecture (Step-by-Step)\n",
    "- **Step 1: Input (Target Word)**\n",
    "  - Single target word (e.g., \"fox\").\n",
    "  - Represented as a one-hot vector (length $V$, vocab size, e.g., 10,000; 1 at word’s index, 0s elsewhere).\n",
    "  - Example: \"fox\" (index 500) → $[0, ..., 1, ..., 0]$.\n",
    "- **Step 2: Embedding Lookup**\n",
    "  - One-hot vector indexes into input embedding matrix $W_{\\text{in}}$ (shape: $V \\times d$, $d = 100$).\n",
    "  - Output: Dense target vector $x$ (d-dimensional).\n",
    "  - Example: $x$ for \"fox\" = $[0.3, -0.2, ...]$.\n",
    "- **Step 3: Output Layer**\n",
    "  - Multiply $x$ by output embedding matrix $W_{\\text{out}}$ (shape: $d \\times V$) to get scores: $z = x \\cdot W_{\\text{out}}$ (length $V$).\n",
    "  - Softmax: $\\hat{y}_j = \\frac{\\exp(z_j)}{\\sum_k \\exp(z_k)}$ (probabilities for all context words).\n",
    "- **Step 4: Training**\n",
    "  - Loss: Cross-entropy, $\\mathcal{L} = -\\sum_{j \\in \\text{context}} \\log \\hat{y}_j$ (maximize context word probabilities).\n",
    "  - Optimize $W_{\\text{in}}$, $W_{\\text{out}}$ via backpropagation.\n",
    "  - Final embeddings: Rows of $W_{\\text{in}}$ (or average with $W_{\\text{out}}$).\n",
    "- **Parameters**: Approx. $2 \\times V \\times d$ (e.g., 2M for $V = 10,000$, $d = 100$).\n",
    "\n",
    "## 3. Why Dense Embeddings (Not One-Hot)?\n",
    "- **One-Hot Vectors**:\n",
    "  - Length $V$, sparse, no semantic info (all words orthogonal, e.g., \"cat\" ≠ \"dog\").\n",
    "  - Summing context one-hots: Vector with 1s at context word indices (e.g., 1s for \"quick\", \"brown\").\n",
    "  - Issues: High-dimensional ($V$), computationally expensive, no generalization across similar words.\n",
    "- **Dense Embeddings**:\n",
    "  - $d$-dimensional ($d \\ll V$), learned to capture semantics (e.g., \"cat\" ≈ \"dog\").\n",
    "  - Single target embedding $x$ predicts context, leveraging semantic similarity.\n",
    "  - Efficient: Fixed-size input ($d$), enables generalization (similar targets → similar contexts).\n",
    "- **Why Not Summed One-Hot?**: Sparse, no semantic structure, requires larger weight matrix ($V \\times d$) vs. Skip-gram’s efficient lookup.\n",
    "\n",
    "## 4. Why Predicting Multiple Contexts?\n",
    "- Uses single target vector $x$ to predict $C$ context words (e.g., $C = 4$).\n",
    "- **Benefits**:\n",
    "  - Captures relationships from multiple contexts per target, enriching embeddings.\n",
    "  - Fixed-size input ($d$) regardless of $C$, scalable for large windows.\n",
    "  - Order-insensitive within context window, simplifies model.\n",
    "- **Contrast with Averaging**: No averaging (unlike CBOW); each context prediction contributes independently.\n",
    "\n",
    "## 5. Negative Sampling: Why and How?\n",
    "- **Problem**: Softmax over $V$ words (O($V$)) is slow for large vocab (e.g., $V = 10,000$).\n",
    "- **Solution**: Negative sampling:\n",
    "  - For each context word: Use target-context pair (positive) + $k$ negative words (e.g., $k = 5–20$, sampled by frequency^0.75).\n",
    "  - Loss: $\\mathcal{L} = -\\log \\sigma(x \\cdot v_{c,\\text{out}}) - \\sum_{i=1}^k \\log \\sigma(-x \\cdot v_{n_i,\\text{out}})$ per context word.\n",
    "  - Compute scores for only $1 + k$ words, complexity O($k$) vs. O($V$).\n",
    "- **$W_{\\text{out}}$ Dimensions**:\n",
    "  - Remains $d \\times V$, as all words can be contexts/negatives.\n",
    "  - Each step uses subset ($d \\times (1 + k)$) for context + $k$ negatives.\n",
    "  - Full matrix updated over time as different words are sampled.\n",
    "- **Why Not $d \\times k$?**: Would cover only $k$ words, insufficient for vocab size $V$.\n",
    "\n",
    "## 6. Why Skip-gram Works?\n",
    "- **Semantic Learning**: Predicting contexts from a target groups similar words (e.g., \"king\" ≈ \"queen\") via distributional hypothesis.\n",
    "- **Dot Product Theory**:\n",
    "  - Output scores ($z = x \\cdot W_{\\text{out}}$) are dot products between target vector $x$ and each context’s output embedding ($v_{j,\\text{out}}$, columns of $W_{\\text{out}}$).\n",
    "  - Dot product measures similarity: High $x \\cdot v_{c,\\text{out}}$ means target aligns with context word’s embedding; low for unrelated words.\n",
    "  - Training maximizes $x \\cdot v_{c,\\text{out}}$ (context) and minimizes $x \\cdot v_{n,\\text{out}}$ (negatives), positioning similar words close in embedding space.\n",
    "  - Result: Embeddings capture semantic relationships (e.g., \"cat\" ≈ \"dog\") and enable analogies (e.g., \"king - man + woman ≈ queen\") via linear vector arithmetic.\n",
    "- **Dense Embeddings**: Low-dimensional ($d$), capture latent features (e.g., gender, topic), enable generalization across diverse contexts.\n",
    "- **Multiple Contexts**: Predicting $C$ contexts per target enriches embeddings, especially for rare words.\n",
    "- **Large-Scale Training**: Millions of examples ensure robust embeddings, capturing diverse target-context pairs.\n",
    "- **Efficiency**: Negative sampling reduces computation, making large vocabularies feasible.\n",
    "- **Simplicity**: Linear model (no non-linearities except softmax) learns complex patterns via data volume.\n",
    "\n",
    "## 7. Key Numbers (Example)\n",
    "- Vocabulary: $V = 10,000$.\n",
    "- Embedding dim: $d = 100$.\n",
    "- Context size: $C = 4$.\n",
    "- Negative samples: $k = 5–20$.\n",
    "- Parameters: Approx. $2 \\times V \\times d$ (e.g., 2M for $V = 10,000$, $d = 100$).\n",
    "- Complexity: O($d + d \\times k$) per context word with negative sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887f094c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
