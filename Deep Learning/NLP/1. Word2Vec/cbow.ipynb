{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09f0bded",
   "metadata": {},
   "source": [
    "# CBOW Word2Vec Revision Notebook\n",
    "\n",
    "A one-page flashcard-style revision tool for the Continuous Bag of Words (CBOW) model in Word2Vec. Covers architecture, why it works (including dot product theory), dense embeddings vs. one-hot vectors, and negative sampling. Designed for quick, deep review without code.\n",
    "\n",
    "## 1. What is CBOW?\n",
    "- **Definition**: Predicts a target word (e.g., \"fox\") from its context words (e.g., \"quick\", \"brown\", \"jumps\", \"over\").\n",
    "- **Goal**: Learn dense word embeddings (vectors, e.g., 100-dim) that capture semantic/syntactic relationships (e.g., \"king\" ≈ \"queen\", \"king - man + woman ≈ queen\").\n",
    "- **Core Idea**: Distributional hypothesis—words in similar contexts have similar meanings.\n",
    "- **Training**: Unsupervised, uses large text corpora (e.g., Wikipedia), sliding window to generate (context, target) pairs.\n",
    "- **Example**: Sentence: \"The quick brown fox jumps.\" Context (window size 2): \"quick\", \"brown\", \"jumps\", \"over\". Target: \"fox\".\n",
    "\n",
    "## 2. CBOW Architecture (Step-by-Step)\n",
    "- **Step 1: Input (Context Words)**\n",
    "  - $C$ context words (e.g., $C = 4$).\n",
    "  - Each word as a one-hot vector (length $V$, vocab size, e.g., 10,000; 1 at word’s index, 0s elsewhere).\n",
    "  - Example: \"quick\" (index 100) → $[0, ..., 1, ..., 0]$.\n",
    "- **Step 2: Embedding Lookup**\n",
    "  - One-hot vectors index into input embedding matrix $W_{\\text{in}}$ (shape: $V \\times d$, $d = 100$).\n",
    "  - Output: $C$ dense vectors $x_1, x_2, ..., x_C$, each $d$-dimensional.\n",
    "  - Example: $x_1$ for \"quick\" = $[0.2, -0.1, ...]$.\n",
    "- **Step 3: Context Aggregation**\n",
    "  - Take embeddings of the context words and average elementwise: $h = \\frac{1}{C} \\sum x_i$ ($d$-dimensional context vector).\n",
    "  - Represents combined semantic content, order-insensitive (\"bag of words\", also called \"continuous\" because it's dense and not 1 hot sparse vector).\n",
    "- **Step 4: Output Layer**\n",
    "  - Multiply $h$ by output embedding matrix $W_{\\text{out}}$ (shape: $d \\times V$) to get scores: $z = h \\cdot W_{\\text{out}}$ (length $V$).\n",
    "  - Softmax: $\\hat{y}_j = \\frac{\\exp(z_j)}{\\sum_k \\exp(z_k)}$ (probabilities for all words).\n",
    "- **Step 5: Training**\n",
    "  - Loss: Cross-entropy, $\\mathcal{L} = -\\log \\hat{y}_t$ (maximize target word probability).\n",
    "  - Optimize $W_{\\text{in}}$, $W_{\\text{out}}$ via backpropagation.\n",
    "  - Final Product embeddings: Rows of $W_{\\text{in}}$ (or average with $W_{\\text{out}}$).\n",
    "- **Parameters**: Approx. $2 \\times V \\times d$ (e.g., 2M for $V = 10,000$, $d = 100$).\n",
    "\n",
    "## 3. Why Dense Embeddings (Not One-Hot)?\n",
    "- **One-Hot Vectors**:\n",
    "  - Length $V$, sparse, no semantic info (all words orthogonal, e.g., \"cat\" ≠ \"dog\").\n",
    "  - Issues: High-dimensional ($V$), computationally expensive, no generalization across similar words.\n",
    "- **Dense Embeddings**:\n",
    "  - $d$-dimensional ($d \\ll V$), learned to capture semantics (e.g., \"cat\" ≈ \"dog\").\n",
    "  - Averaging embeddings blends meaning (e.g., \"cat\" + \"dog\" → pet-related vector).\n",
    "  - Efficient: Fixed-size input ($d$), enables generalization (similar contexts → similar $h$).\n",
    "- **Why Not Summed One-Hot?**: Sparse, no semantic structure, requires larger weight matrix ($V \\times d$) vs. CBOW’s efficient lookup and averaging.\n",
    "\n",
    "## 4. Why Averaging Context Embeddings?\n",
    "- Combines $C$ embeddings into one $d$-dimensional vector $h$.\n",
    "- **Benefits**:\n",
    "  - Fixed size regardless of $C$ (vs. concatenation → $C \\times d$).\n",
    "  - Captures collective semantic meaning (e.g., \"quick\" + \"brown\" → context for \"fox\").\n",
    "  - Order-insensitive, simplifies model, effective for embeddings.\n",
    "- **Alternative (Summing One-Hots)**: Only tracks word presence, not semantic relationships.\n",
    "\n",
    "## 5. Negative Sampling: Why and How?\n",
    "- **Problem**: Softmax over $V$ words (O($V$)) is slow for large vocab (e.g., $V = 10,000$).\n",
    "- **Solution**: Negative sampling:\n",
    "  - For each example: Use target word (positive) + $k$ negative words (e.g., $k = 5–20$, sampled by frequency^0.75).\n",
    "  - Loss: $\\mathcal{L} = -\\log \\sigma(h \\cdot v_{t,\\text{out}}) - \\sum_{i=1}^k \\log \\sigma(-h \\cdot v_{n_i,\\text{out}})$.\n",
    "  - Compute scores for only $1 + k$ words, complexity O($k$) vs. O($V$).\n",
    "- **$W_{\\text{out}}$ Dimensions**:\n",
    "  - Remains $d \\times V$, as all words can be targets/negatives.\n",
    "  - Each step uses subset ($d \\times (1 + k)$) for target + $k$ negatives.\n",
    "  - Full matrix updated over time as different words are sampled.\n",
    "- **Why Not $d \\times k$?**: Would cover only $k$ words, insufficient for vocab size $V$.\n",
    "\n",
    "## 6. Why CBOW Works?\n",
    "- **Semantic Learning**: Context-based prediction groups similar words (e.g., \"king\" ≈ \"queen\") via distributional hypothesis.\n",
    "- **Dot Product Theory**:\n",
    "  - Output scores ($z = h \\cdot W_{\\text{out}}$) are dot products between context vector $h$ (averaged input embeddings) and each word’s output embedding ($v_{j,\\text{out}}$, columns of $W_{\\text{out}}$).\n",
    "  - Dot product measures cosine similarity: High $h \\cdot v_{t,\\text{out}}$ means context aligns with target word’s embedding; low for unrelated words.\n",
    "  - Training maximizes $h \\cdot v_{t,\\text{out}}$ (target) and minimizes $h \\cdot v_{n,\\text{out}}$ (others), positioning similar words close in embedding space.\n",
    "  - Result: Embeddings capture semantic relationships (e.g., \"cat\" ≈ \"dog\") and enable analogies (e.g., \"king - man + woman ≈ queen\") via linear vector arithmetic.\n",
    "- **Dense Embeddings**: Low-dimensional ($d$), capture latent features (e.g., gender, topic), enable generalization across similar contexts.\n",
    "- **Averaging**: Blends context meaning efficiently, scalable for any $C$, produces $h$ that reflects collective semantics.\n",
    "- **Large-Scale Training**: Millions of examples ensure robust embeddings, capturing diverse contexts.\n",
    "- **Efficiency**: Negative sampling reduces computation, making large vocabularies feasible.\n",
    "- **Simplicity**: Linear model (no non-linearities except softmax) learns complex patterns via data volume.\n",
    "\n",
    "## 7. Key Numbers (Example)\n",
    "- Vocabulary: $V = 10,000$.\n",
    "- Embedding dim: $d = 100$.\n",
    "- Context size: $C = 4$.\n",
    "- Negative samples: $k = 5–20$.\n",
    "- Parameters: Approx. $2 \\times V \\times d$ (e.g., 2M for $V = 10,000$, $d = 100$).\n",
    "- Complexity: O($C \\times d + d \\times k$) per example with negative sampling."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
