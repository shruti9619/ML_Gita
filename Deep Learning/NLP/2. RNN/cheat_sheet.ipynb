{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d73e399",
   "metadata": {},
   "source": [
    "# RNN Revision Notebook\n",
    "\n",
    "A one-page flashcard-style revision tool explaining Recurrent Neural Networks (RNNs), their architecture, variants, and applications like sentiment analysis. Designed for quick, deep review without code.\n",
    "\n",
    "## 1. What Are RNNs?\n",
    "- **Definition**: Neural networks designed for sequential data, maintaining a \"memory\" of previous inputs to process sequences (e.g., sentences, time series).\n",
    "- **Goal**: Learn patterns over time, unlike Word2Vec’s static embeddings.\n",
    "- **Example**: Predicting next word in \"I am happy\" or sentiment from \"not good\".\n",
    "- **Key Idea**: Same weights applied across time steps, sharing information.\n",
    "\n",
    "## 2. RNN Architecture (Step-by-Step)\n",
    "- **Input**: $x_t$ at time $t$ (e.g., word vector from W2V).\n",
    "- **Hidden State**: $h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b)$, where:\n",
    "  - $h_{t-1}$: Previous hidden state (memory).\n",
    "  - $W_{xh}$: Input-to-hidden weights.\n",
    "  - $W_{hh}$: Hidden-to-hidden weights.\n",
    "  - $f$: Activation (e.g., tanh) for non-linearity.\n",
    "  - $b$: Bias.\n",
    "- **Output**: $y_t = W_{hy} h_t + b_y$ (e.g., sentiment score), optional per step.\n",
    "- **Flow**: Loops back $h_t$ to next step, processing sequence left-to-right.\n",
    "- **Parameters**: Shared $W_{xh}$, $W_{hh}$, $W_{hy}$, efficient for long sequences.\n",
    "\n",
    "## 3. Loss Calculation and Backpropagation\n",
    "- **Loss Function**: Typically cross-entropy for classification (e.g., sentiment):\n",
    "  - $L = -\\frac{1}{T} \\sum_{t=1}^T [y_t \\log \\hat{y}_t + (1 - y_t) \\log (1 - \\hat{y}_t)]$, where $y_t$ is true label, $\\hat{y}_t$ is predicted probability.\n",
    "  - For sequence, sums over all time steps $T$.\n",
    "- **Backpropagation Through Time (BPTT)**:\n",
    "  - Unrolls RNN over $T$ steps into a feedforward network.\n",
    "  - Computes gradients of $L$ w.r.t. $W_{xh}$, $W_{hh}$, $W_{hy}$ by chain rule across time.\n",
    "  - Gradient for $W_{hh}$: $\\frac{\\partial L}{\\partial W_{hh}} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial W_{hh}}$, accumulating over steps.\n",
    "- **Challenges**:\n",
    "  - Vanishing gradients: Small gradients multiply over $T$, weakening early updates.\n",
    "  - Exploding gradients: Large gradients require clipping.\n",
    "  - Solution: Truncate BPTT (e.g., backprop over 5-10 steps) or use LSTMs.\n",
    "\n",
    "## 4. How RNNs Work for Sentiment Analysis\n",
    "- **Sequence Processing**: Reads words one-by-one (e.g., \"I\", \"am\", \"not\", \"good\").\n",
    "- **Memory**: $h_t$ accumulates context; \"not\" flips \"good\" to negative.\n",
    "- **Output**: Final $h_t$ or $y_t$ fed to classifier (e.g., softmax) for positive/negative.\n",
    "- **Advantage**: Captures order, unlike W2V’s bag-of-words assumption.\n",
    "\n",
    "## 5. Limitations of Basic RNNs\n",
    "- **Vanishing Gradients**: Long-term dependencies (e.g., \"the cat that ate\" → \"food\") fade due to repeated multiplication of gradients.\n",
    "- **Exploding Gradients**: Unstable training with large weight updates, requiring clipping.\n",
    "- **Sequential Nature**: Processes one step at a time, slow for long sequences.\n",
    "- **Short Memory**: Struggles beyond ~10 words without fixes.\n",
    "\n",
    "## 6. Advanced RNN Variants\n",
    "- **LSTM (Long Short-Term Memory)**:\n",
    "  - Adds gates: Forget ($f_t$), Input ($i_t$), Output ($o_t$) to control memory.\n",
    "  - Cell state $c_t = f_t \\cdot c_{t-1} + i_t \\cdot \\tilde{c}_t$ retains long-term info.\n",
    "  - Sentiment Benefit: Remembers \"not\" over sentences.\n",
    "- **GRU (Gated Recurrent Unit)**:\n",
    "  - Simplified LSTM with update ($z_t$) and reset ($r_t$) gates.\n",
    "  - $h_t = (1 - z_t) \\cdot h_{t-1} + z_t \\cdot \\tilde{h}_t$, faster but less flexible.\n",
    "  - Sentiment Benefit: Efficient for shorter texts.\n",
    "- **Why Use**: Solve vanishing gradients, handle longer dependencies.\n",
    "\n",
    "## 7. Why RNNs Matter\n",
    "- **Bridge to Attention**: Memory ($h_t$) and sequence handling inspire attention mechanisms.\n",
    "- **Pre-Transformer**: Base for bidirectional RNNs, leading to Transformers.\n",
    "- **Sentiment Evolution**: From W2V’s static embeddings to RNN’s dynamic context, then attention/Transformers.\n",
    "- **Practice**: Apply to \"I am not happy\" to see order impact.\n",
    "\n",
    "## 8. Key Points (Milestones)\n",
    "- **Structure**: Single layer with loops, ~1980s concept, popularized 2010s.\n",
    "- **Dims**: Hidden state $h_t$ typically 100-512 dims.\n",
    "- **Training**: Backpropagation through time (BPTT), gradient issues common.\n",
    "- **Next Step**: Learn LSTMs/GRUs, then attention for Transformers.\n",
    "- **Sentiment**: ~85% accuracy on SST-2 with LSTMs, vs. ~80% with W2V."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
