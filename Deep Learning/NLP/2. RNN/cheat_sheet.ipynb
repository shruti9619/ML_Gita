{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d73e399",
   "metadata": {},
   "source": [
    "# RNN Revision Notebook\n",
    "\n",
    "A one-page flashcard-style revision tool explaining Recurrent Neural Networks (RNNs), their architecture, variants, and applications like sentiment analysis. Designed for quick, deep review without code.\n",
    "\n",
    "## 1. What Are RNNs?\n",
    "- **Definition**: Neural networks designed for sequential data, maintaining a \"memory\" of previous inputs to process sequences (e.g., sentences, time series).\n",
    "- **Goal**: Learn patterns over time, unlike Word2Vec’s static embeddings.\n",
    "- **Example**: Predicting next word in \"I am happy\" or sentiment from \"not good\".\n",
    "- **Key Idea**: Same weights applied across time steps, sharing information.\n",
    "\n",
    "## 2. RNN Architecture (Step-by-Step)\n",
    "- **Input**: $x_t$ at time $t$ (e.g., word vector from W2V).\n",
    "- **Hidden State**: $h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b)$, where:\n",
    "  - $h_{t-1}$: Previous hidden state (memory).\n",
    "  - $W_{xh}$: Input-to-hidden weights.\n",
    "  - $W_{hh}$: Hidden-to-hidden weights.\n",
    "  - $f$: Activation (e.g., tanh) for non-linearity.\n",
    "  - $b$: Bias.\n",
    "- **Output**: $y_t = W_{hy} h_t + b_y$ (e.g., sentiment score), optional per step.\n",
    "- **Flow**: Loops back $h_t$ to next step, processing sequence left-to-right.\n",
    "- **Parameters**: Shared $W_{xh}$, $W_{hh}$, $W_{hy}$, efficient for long sequences.\n",
    "\n",
    "## 3. Loss Calculation and Backpropagation\n",
    "- **Loss Function**: Typically cross-entropy for classification (e.g., sentiment):\n",
    "  - $L = -\\frac{1}{T} \\sum_{t=1}^T [y_t \\log \\hat{y}_t + (1 - y_t) \\log (1 - \\hat{y}_t)]$, where $y_t$ is true label, $\\hat{y}_t$ is predicted probability.\n",
    "  - For sequence, sums over all time steps $T$.\n",
    "- **Backpropagation Through Time (BPTT)**:\n",
    "  - Unrolls RNN over $T$ steps into a feedforward network.\n",
    "  - Computes gradients of $L$ w.r.t. $W_{xh}$, $W_{hh}$, $W_{hy}$ by chain rule across time.\n",
    "  - Gradient for $W_{hh}$: $\\frac{\\partial L}{\\partial W_{hh}} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial W_{hh}}$, accumulating over steps.\n",
    "- **Challenges**:\n",
    "  - Vanishing gradients: Small gradients multiply over $T$, weakening early updates.\n",
    "  - Exploding gradients: Large gradients require clipping.\n",
    "  - Solution: Truncate BPTT (e.g., backprop over 5-10 steps) or use LSTMs.\n",
    "\n",
    "## 4. How RNNs Work for Sentiment Analysis\n",
    "- **Sequence Processing**: Reads words one-by-one (e.g., \"I\", \"am\", \"not\", \"good\").\n",
    "- **Memory**: $h_t$ accumulates context; \"not\" flips \"good\" to negative.\n",
    "- **Output**: Final $h_t$ or $y_t$ fed to classifier (e.g., softmax) for positive/negative.\n",
    "- **Advantage**: Captures order, unlike W2V’s bag-of-words assumption.\n",
    "\n",
    "## 5. Limitations of Basic RNNs\n",
    "- **Vanishing Gradients**: Long-term dependencies (e.g., \"the cat that ate\" → \"food\") fade due to repeated multiplication of gradients.\n",
    "- **Exploding Gradients**: Unstable training with large weight updates, requiring clipping.\n",
    "- **Sequential Nature**: Processes one step at a time, slow for long sequences.\n",
    "- **Short Memory**: Struggles beyond ~10 words without fixes.\n",
    "\n",
    "## 6. Advanced RNN Variants\n",
    "- **LSTM (Long Short-Term Memory)**:\n",
    "  - Adds gates: Forget ($f_t$), Input ($i_t$), Output ($o_t$) to control memory.\n",
    "  - Cell state $c_t = f_t \\cdot c_{t-1} + i_t \\cdot \\tilde{c}_t$ retains long-term info.\n",
    "  - Sentiment Benefit: Remembers \"not\" over sentences.\n",
    "- **GRU (Gated Recurrent Unit)**:\n",
    "  - Simplified LSTM with update ($z_t$) and reset ($r_t$) gates.\n",
    "  - $h_t = (1 - z_t) \\cdot h_{t-1} + z_t \\cdot \\tilde{h}_t$, faster but less flexible.\n",
    "  - Sentiment Benefit: Efficient for shorter texts.\n",
    "- **Why Use**: Solve vanishing gradients, handle longer dependencies.\n",
    "\n",
    "## 7. Why RNNs Matter\n",
    "- **Bridge to Attention**: Memory ($h_t$) and sequence handling inspire attention mechanisms.\n",
    "- **Pre-Transformer**: Base for bidirectional RNNs, leading to Transformers.\n",
    "- **Sentiment Evolution**: From W2V’s static embeddings to RNN’s dynamic context, then attention/Transformers.\n",
    "- **Practice**: Apply to \"I am not happy\" to see order impact.\n",
    "\n",
    "## 8. Key Points (Milestones)\n",
    "- **Structure**: Single layer with loops, ~1980s concept, popularized 2010s.\n",
    "- **Dims**: Hidden state $h_t$ typically 100-512 dims.\n",
    "- **Training**: Backpropagation through time (BPTT), gradient issues common.\n",
    "- **Next Step**: Learn LSTMs/GRUs, then attention for Transformers.\n",
    "- **Sentiment**: ~85% accuracy on SST-2 with LSTMs, vs. ~80% with W2V."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43170baf",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370e0cc",
   "metadata": {},
   "source": [
    "# RNN vs. Word2Vec Revision Notebook\n",
    "\n",
    "A one-page flashcard-style revision tool explaining why RNNs outperform Word2Vec (W2V) for certain tasks, with applications and limitations. Designed for quick, deep review without code.\n",
    "\n",
    "## 1. What Are RNNs and Word2Vec?\n",
    "- **Word2Vec (W2V)**: Unsupervised model creating static word embeddings (e.g., 300 dims) based on word co-occurrences, capturing semantic similarity.\n",
    "- **RNNs**: Neural networks processing sequences with memory ($h_t$), handling dynamic context over time.\n",
    "- **Context**: W2V is a static embedding tool; RNNs are dynamic sequence models.\n",
    "\n",
    "## 2. Why RNNs Are Better Than Word2Vec\n",
    "- **Dynamic Context**:\n",
    "  - W2V: Fixed embedding per word (e.g., \"bank\" same for river/money), ignores context.\n",
    "  - RNN: Updates $h_t$ per step (e.g., \"not\" changes \"good\" to negative), capturing order.\n",
    "  - Example: \"I am not happy\" → W2V misses negation, RNN adjusts sentiment.\n",
    "- **Sequence Handling**:\n",
    "  - W2V: Treats text as bag-of-words, losing sequence (e.g., \"dog bites man\" vs. \"man bites dog\").\n",
    "  - RNN: Processes word-by-word, preserving order for tasks like translation.\n",
    "  - Example: Sentiment in \"great, but boring\" needs sequence awareness.\n",
    "- **Longer Dependencies**:\n",
    "  - W2V: No memory, can’t link distant words (e.g., \"the cat\" to \"food\").\n",
    "  - RNN: Retains context in $h_t$ (with limits), better for longer texts.\n",
    "  - Example: Review \"loved the start, hated the end\" → RNN tracks shift.\n",
    "- **Task Flexibility**:\n",
    "  - W2V: Limited to word-level features, needs extra models for tasks.\n",
    "  - RNN: End-to-end training for classification, generation (e.g., sentiment labels).\n",
    "  - Example: W2V + classifier vs. RNN direct prediction.\n",
    "- **Empirical Gain**: RNNs achieve ~85% on SST-2 vs. W2V’s ~80% with added layers.\n",
    "\n",
    "## 3. How RNNs Work Better for Sentiment Analysis\n",
    "- **Contextual Nuance**: RNN’s $h_t$ adapts to \"not good\" as negative, W2V’s \"good\" stays positive.\n",
    "- **Variable Input**: Handles reviews of any length, W2V needs fixed input aggregation.\n",
    "- **Output Generation**: RNN can output sequences (e.g., sentiment explanation), W2V can’t.\n",
    "- **Example**: \"The plot was weak but acting saved it\" → RNN balances, W2V averages.\n",
    "\n",
    "## 4. Limitations Where W2V Might Suffice\n",
    "- **Simplicity**: W2V is faster, precomputed, ideal for small datasets or word similarity tasks.\n",
    "- **Static Need**: If context isn’t key (e.g., keyword search), W2V’s fixed embeddings work.\n",
    "- **Resource Constraints**: RNNs need more computation/memory, W2V is lightweight.\n",
    "- **Example**: Word analogy (\"king - man + woman ≈ queen\") suits W2V.\n",
    "\n",
    "## 6. Key Points (Comparison)\n",
    "- **Strength**: RNNs handle sequence, context, flexibility; W2V excels in simplicity, speed.\n",
    "- **Accuracy**: RNN ~85%, W2V ~80% on sentiment with extras.\n",
    "- **Use Case**: RNN for dynamic tasks (sentiment, translation); W2V for static word tasks.\n",
    "- **Limit**: RNN’s vanishing gradients vs. W2V’s no-sequence memory.\n",
    "- **Evolution**: RNNs pave way for seq2seq and Transformers."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
