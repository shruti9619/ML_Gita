{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees Classifier\n",
    "\n",
    "Extra Trees, or Extremely Randomized Trees, is an ensemble machine learning algorithm that combines predictions from many decision trees. It is similar to the widely-used random forest algorithm but often achieves equal or better performance with a simpler tree construction algorithm.\n",
    "\n",
    "#### Introduction\n",
    "Extra Trees is an ensemble of decision trees related to other ensemble methods like bagging and random forests. It builds multiple unpruned decision trees from the training dataset. Predictions are made by:\n",
    "\n",
    "- **Regression**: Averaging the predictions from all trees.\n",
    "- **Classification**: Using majority voting among the trees.\n",
    "\n",
    "#### Key Differences from Random Forest\n",
    "\n",
    "- **Training Dataset**: Unlike bagging and random forest, which develop each decision tree from a bootstrap sample of the training dataset, Extra Trees fits each decision tree on the whole training dataset.\n",
    "- **Split Points**: Random forest uses a greedy algorithm to select the optimal split point. Extra Trees, however, selects split points at random, which makes the decision trees in the ensemble less correlated.\n",
    "\n",
    "#### Variance and Algorithm Comparison\n",
    "Ensemble methods can be ordered from high to low variance:\n",
    "\n",
    "1. **Decision Tree (High Variance)**: A single decision tree often overfits the training data and makes less accurate predictions on new data.\n",
    "2. **Random Forest (Medium Variance)**: Random forest models reduce overfitting by:\n",
    "   - Building multiple trees (`n_estimators`).\n",
    "   - Drawing observations with replacement (bootstrap sample).\n",
    "   - Splitting nodes on the best split among a random subset of features at every node.\n",
    "3. **Extra Trees (Low Variance)**: Extra Trees reduces variance by:\n",
    "   - Building multiple trees with `bootstrap=False` (sampling without replacement).\n",
    "   - Splitting nodes on random splits among a random subset of features at every node.\n",
    "\n",
    "#### Summary of Extra Trees\n",
    "- **Bootstrap**: By default, `bootstrap=False`, meaning it samples without replacement.\n",
    "- **Random Splits**: Nodes are split based on random splits among a random subset of features.\n",
    "\n",
    "In Extra Trees, randomness is introduced **not** through bootstrapping but through random splits. **Using the whole original sample instead of a bootstrap replica reduces bias, while random split points reduce variance.** The Extra Trees algorithm is computationally faster and named for its extreme randomization of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation** \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
