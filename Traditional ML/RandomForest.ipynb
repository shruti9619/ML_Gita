{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "The Random Forest algorithm is a powerful and versatile machine learning method that is used for both classification and regression tasks. As an ensemble learning model, it operates by constructing a multitude of decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "But before we dive into this algorithm let's learn a bit about **Bagging** or **Bootstrap Aggregation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Bootstrap Method\n",
    "\n",
    "The bootstrap is a powerful statistical technique for estimating a quantity from a data sample. This is easiest to understand when the quantity is a descriptive statistic, such as a mean or standard deviation.\n",
    "\n",
    "Suppose we have a sample of 100 values (x) and want to estimate the mean of the sample.\n",
    "\n",
    "We can calculate the mean directly from the sample as:\n",
    "\n",
    "\\[ \\text{mean}(x) = \\frac{1}{100} \\sum x \\]\n",
    "\n",
    "However, we know that our sample is small and our mean has some error. We can improve our mean estimate using the bootstrap procedure:\n",
    "\n",
    "1. Create many (e.g., 1000) random sub-samples of our dataset with replacement (meaning a value can be selected multiple times).\n",
    "2. Calculate the mean of each sub-sample.\n",
    "3. Calculate the average of all the sub-sample means and use that as our estimated mean for the data.\n",
    "\n",
    "For instance, let's say we used 3 resamples and obtained mean values of 2.3, 4.5, and 3.3. By averaging these, we get an estimated mean of 3.367.\n",
    "\n",
    "This process can also estimate other quantities, such as the standard deviation, and even parameters used in machine learning algorithms, like learned coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Aggregation (Bagging)\n",
    "\n",
    "Bootstrap Aggregation, commonly known as Bagging, is a straightforward yet powerful ensemble method. \n",
    "\n",
    "An ensemble method combines predictions from multiple machine learning algorithms to produce more accurate results than any single model could achieve on its own.\n",
    "\n",
    "Bagging is a general technique used to reduce the variance of high-variance algorithms, such as decision trees (e.g., Classification and Regression Trees, or CART).\n",
    "\n",
    "Decision trees are particularly sensitive to the specific data on which they are trained. If the training data changes (e.g., training on different subsets of the data), the resulting trees and their predictions can vary significantly.\n",
    "\n",
    "Bagging involves applying the Bootstrap procedure to high-variance algorithms, typically decision trees. This helps in stabilizing the predictions and improving the overall model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Bagging with the CART Algorithm\n",
    "\n",
    "Assume we have a sample dataset of 1000 instances (x) and we are using the Classification and Regression Trees (CART) algorithm. Here's how Bagging with the CART algorithm would work:\n",
    "\n",
    "1. Create many (e.g., 100) random sub-samples of our dataset with replacement.\n",
    "2. Train a CART model on each sub-sample.\n",
    "3. For a new dataset, calculate the average prediction from each model.\n",
    "\n",
    "For example, if we had 5 bagged decision trees that made the following class predictions for an input sample: blue, blue, red, blue, and red, we would take the most frequent class and predict blue.\n",
    "\n",
    "### Bagging with Decision Trees\n",
    "\n",
    "When bagging with decision trees, we are less concerned about individual trees overfitting the training data. To improve efficiency and ensure variety among the trees, we typically grow individual decision trees deep (i.e., with few training samples at each leaf node) and do not prune them. These deep trees have both high variance and low bias, which are crucial characteristics for sub-models in a bagging ensemble.\n",
    "\n",
    "A very deep decision tree tends to learn the noise in the data, leading to overfitting. This results in low bias but high variance. Pruning is usually applied to reduce overfitting in decision trees, but with bagging, we leverage the high variance of individual trees to enhance the ensemble's overall performance.\n",
    "\n",
    "### The Edge with Random Forest \n",
    "\n",
    "So now we know that Decision Trees can be used in ensemble model aka with bagging technique to help in generalization and reduce variance in other terms. Then why another algorithm named Random forest? Is it any different from bagging with Decision Trees?\n",
    "The answer is **yes**. Random Forest algorithm is an extension of bagging with Decision Trees, it combines bootstrap aggregation (sampling with replacement) with random feature selection for splits, which reduces the correlation between trees. \n",
    "\n",
    "Even with Bagging, decision trees can exhibit structural similarities, leading to high correlation in their predictions.\n",
    "Combining predictions from multiple models in ensembles works best when the predictions from the sub-models are uncorrelated or only weakly correlated.\n",
    "\n",
    "Random Forests modify the algorithm for learning sub-trees, resulting in less correlation between their predictions.\n",
    "\n",
    "Here's how the two are different:\n",
    "\n",
    "- **Bagged CART Algorithm:** In the standard CART algorithm, when selecting a split point, the learning algorithm considers all variables and all variable values to choose the most optimal split point.\n",
    "- **Random Forest Algorithm:** The Random Forest algorithm tweaks this procedure by restricting the learning algorithm to a random sample of features to search for the split point.\n",
    "\n",
    "This simple change reduces the correlation among the sub-trees' predictions, leading to a more robust ensemble model.\n",
    "\n",
    "This means that although individual trees have high variance, the ensemble output is more stable (lower variance and lower bias) due to the low correlation between the trees. This results in a more accurate and reliable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
